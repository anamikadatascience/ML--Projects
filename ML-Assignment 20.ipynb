{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40efb18b",
   "metadata": {},
   "source": [
    "1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2cd567",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    Support Vector Machines (SVMs) are a machine learning technique that finds a hyperplane to best separate different classes of data by maximizing the margin between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46cd98",
   "metadata": {},
   "source": [
    "2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a841d38",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    A support vector is a data point that lies closest to the decision boundary (hyperplane) of a Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fa864",
   "metadata": {},
   "source": [
    "3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c71d64",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    Scaling inputs in SVMs is necessary to ensure that all features contribute equally to the decision boundary calculation and prevent one feature from dominating others due to its larger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b36130",
   "metadata": {},
   "source": [
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c968a8f",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    Yes, an SVM classifier can output a confidence score, often referred to as a \"distance from the decision boundary.\" However, it doesn't directly provide a percentage chance or probability estimate like some other classifiers (e.g., logistic regression or neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd7ac7",
   "metadata": {},
   "source": [
    "5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e546e",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    For training with a large dataset and high-dimensional features, the dual form of the SVM problem is generally more efficient and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1caf4",
   "metadata": {},
   "source": [
    "6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b0b4f",
   "metadata": {},
   "source": [
    "Ans\n",
    ":If an SVM classifier with an RBF kernel underfits the training data, you should increase the value of gamma to make the kernel more sensitive to local variations. As for the C parameter, you should decrease it to allow the classifier to have a wider margin and potentially fit the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8fc98",
   "metadata": {},
   "source": [
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3664e89",
   "metadata": {},
   "source": [
    "Ans:n the soft margin linear SVM problem for QP solver:\n",
    "\n",
    "H: Matrix based on the data's inner products and regularization parameter.\n",
    "f: Vector of negative ones.\n",
    "A: Matrix with labels and support vector coefficients.\n",
    "b: Vector of zeros.\n",
    "These parameters are set to formulate the optimization problem that finds the optimal hyperplane while considering the margin and classification errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f216b56",
   "metadata": {},
   "source": [
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bc124",
   "metadata": {},
   "source": [
    "ANS:\n",
    "    LinearSVC, SVC, and SGDClassifier are different algorithms, but they can be tuned to produce similar models on a linearly separable dataset:\n",
    "\n",
    "LinearSVC: Trained with a linear kernel. Use C parameter for regularization.\n",
    "\n",
    "SVC: Trained with a linear kernel. Use a large C parameter for less regularization.\n",
    "\n",
    "SGDClassifier: Use the 'hinge' loss function and tune the learning rate and regularization parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea05b3",
   "metadata": {},
   "source": [
    "9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e8518",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    Training an SVM classifier on the MNIST dataset using one-versus-the-rest strategy and hyperparameter tuning with small validation sets can achieve a precision (accuracy) in the range of 90% to 98% depending on the specific hyperparameters and the amount of training data used. The exact precision will vary based on factors like kernel choice, regularization, and tuning strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeab285",
   "metadata": {},
   "source": [
    "10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02065b62",
   "metadata": {},
   "source": [
    "Training an SVM regressor on the California housing dataset involves using the SVM algorithm to predict continuous target values (house prices) based on the input features (housing-related factors like location, rooms, etc.). The regressor aims to find a hyperplane that best fits the data points. Proper hyperparameter tuning and feature scaling are essential for optimal performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
