{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4fb03f",
   "metadata": {},
   "source": [
    "1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ca660",
   "metadata": {},
   "source": [
    "Ans:\n",
    "The estimated depth of a Decision Tree trained on a one million instance training set can vary widely, but it's common to see depths between 20 to 50 for such a dataset. Keep in mind that this is a rough estimate, as the actual depth can depend on factors such as the complexity of the data and the tree's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c1967",
   "metadata": {},
   "source": [
    "2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da2803",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    The Gini impurity of a node is usually lower than that of its parent. It's not always guaranteed to be lower, but the splitting process in a Decision Tree typically aims to reduce impurity, leading to nodes with lower Gini impurity as you move down the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2450f",
   "metadata": {},
   "source": [
    "3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d54df",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    Yes, reducing the max depth of a Decision Tree can be a good idea if the tree is overfitting the training set. By limiting the depth, the tree becomes less complex and less likely to capture noise or irrelevant details in the data. This can help improve the generalization of the model to unseen data and mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd2d67",
   "metadata": {},
   "source": [
    "4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e51760",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    No, scaling the input features is generally not necessary or effective for addressing underfitting in a Decision Tree. Decision Trees are not sensitive to the scale of input features, and underfitting is typically addressed by increasing the tree's complexity (e.g., allowing greater depth, using more features, or adjusting hyperparameters) rather than by scaling features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d542e",
   "metadata": {},
   "source": [
    "5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485f483",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    Training time for Decision Trees is often affected by the number of instances and the complexity of the data. Assuming similar conditions, training a Decision Tree on a dataset with 10 million instances might take around 10 hours, as it's a linear relationship with the number of instances. However, this is a rough estimate and the actual time can vary based on hardware, software optimizations, and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41173049",
   "metadata": {},
   "source": [
    "\n",
    "6. Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c970ba2",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    No, setting presort=True is unlikely to speed up training for a training set with 100,000 instances. Presorting the data can be beneficial for smaller datasets, but for larger datasets like this, it usually introduces more overhead and can slow down the training process. It's often better to rely on the algorithm's default behavior for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14685",
   "metadata": {},
   "source": [
    "7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5adb29",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    a. Create Moons Dataset:\n",
    "Generate the moons dataset using make_moons(n_samples=10000, noise=0.4).\n",
    "\n",
    "b. Split Dataset:\n",
    "Split the dataset into a training and test set using train_test_split().\n",
    "\n",
    "c. Hyperparameter Tuning:\n",
    "Perform hyperparameter tuning using GridSearchCV with different values for max leaf nodes in a DecisionTreeClassifier.\n",
    "\n",
    "d. Train and Evaluate:\n",
    "Train the Decision Tree model using the optimized hyperparameters on the entire training set, then evaluate its performance on the test set. Aim for an accuracy of 85-87%.\n",
    "\n",
    "Please note that the above steps are summarized, and you would need to write code to execute them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace18955",
   "metadata": {},
   "source": [
    "8. Follow these steps to grow a forest:\n",
    "\n",
    "         a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.\n",
    "\n",
    "          b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision        Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "\n",
    "         c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "\n",
    "         d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a63cd",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "    a. Create Subsets:\n",
    "Generate 1,000 subsets of the training set, each containing 100 instances chosen at random, using the ShuffleSplit class from Scikit-Learn.\n",
    "\n",
    "b. Train Individual Trees:\n",
    "Train 1,000 Decision Trees, each on a different subset, using the best hyperparameter values found earlier. Evaluate their performance on the test set. Expect around 80% accuracy due to smaller training sets.\n",
    "\n",
    "c. Majority Voting:\n",
    "Create predictions using all 1,000 Decision Trees for each test instance. Select the most common prediction using SciPy's mode() function to perform majority voting.\n",
    "\n",
    "d. Evaluate Random Forest:\n",
    "Evaluate the majority-vote predictions on the test set. You should achieve slightly higher accuracy (around 0.5 to 1.5 percent higher) compared to the first model. Congratulations, you've built a Random Forest classifier!\n",
    "\n",
    "Please note that these steps are summarized, and actual implementation requires coding and proper use of libraries like Scikit-Learn and SciPy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
