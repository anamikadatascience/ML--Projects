{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eee64fc",
   "metadata": {},
   "source": [
    "1. Recognize the differences between supervised, semi-supervised, and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3001f20",
   "metadata": {},
   "source": [
    "Ans:\n",
    "   1. Supervised Learning:\n",
    "\n",
    "Learning with labeled examples (input data and correct answers).\n",
    "Used for making predictions or classifications.\n",
    "Example: Teaching a model to recognize cats and dogs based on labeled images.\n",
    "\n",
    "2. Semi-Supervised Learning:\n",
    "\n",
    "Mix of labeled and some unlabeled data.\n",
    "Leverages unlabeled data to improve performance.\n",
    "Useful when labeling data is time-consuming.\n",
    "Example: Training a model to categorize customer feedback with a mix of labeled and unlabeled text.\n",
    "\n",
    "3. Unsupervised Learning:\n",
    "\n",
    "Learning without labeled data.\n",
    "Finds patterns, clusters, or structures in data.\n",
    "Example: Grouping similar customer purchase behavior without knowing specific categories beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73e926",
   "metadata": {},
   "source": [
    "2. Describe in detail any five examples of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d69d3",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    \n",
    "Sure, here are five examples of classification problems:\n",
    "\n",
    "1. Email Spam Detection:\n",
    "\n",
    "Problem: Classify incoming emails as either spam or not spam.\n",
    "Input: Email content, sender information, subject.\n",
    "Output: Binary classification - \"spam\" or \"not spam\".\n",
    "\n",
    "2. Image Object Recognition:\n",
    "\n",
    "Problem: Identify objects or entities in images.\n",
    "Input: Pixel values of an image.\n",
    "Output: Multiclass classification - labels for different objects present in the image (e.g., \"cat,\" \"dog,\" \"car,\" \"tree\").\n",
    "\n",
    "3. Sentiment Analysis:\n",
    "\n",
    "Problem: Determine the sentiment (positive, negative, neutral) of a piece of text.\n",
    "Input: Textual content (reviews, tweets, comments).\n",
    "Output: Multiclass classification - sentiment labels (e.g., \"positive,\" \"negative,\" \"neutral\").\n",
    "\n",
    "4. Medical Diagnosis:\n",
    "\n",
    "Problem: Diagnose medical conditions based on patient symptoms and test results.\n",
    "Input: Patient data (symptoms, test results, medical history).\n",
    "Output: Multiclass classification - different medical conditions or disease states.\n",
    "\n",
    "5. Credit Risk Assessment:\n",
    "\n",
    "Problem: Predict whether a loan applicant is likely to default on a loan.\n",
    "Input: Applicant's financial data, credit history, employment status.\n",
    "Output: Binary classification - \"high risk\" or \"low risk\" for loan approval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b4899",
   "metadata": {},
   "source": [
    "3. Describe each phase of the classification process in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff7f9e",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "    \n",
    "The classification process involves several phases. Here's a concise description of each phase:\n",
    "\n",
    "1. Data Collection and Preparation:\n",
    "\n",
    "Gather relevant data for training and testing.\n",
    "Clean, preprocess, and format the data for consistency.\n",
    "Split the data into training and testing sets to assess model performance.\n",
    "\n",
    "2. Feature Selection and Engineering:\n",
    "\n",
    "Choose relevant features that contribute to classification.\n",
    "Transform raw data into suitable features for the model.\n",
    "Handle categorical variables through encoding techniques.\n",
    "\n",
    "3. Model Selection:\n",
    "\n",
    "Choose a suitable classification algorithm (e.g., Decision Trees, SVM, Neural Networks).\n",
    "Consider the problem's nature, data size, and complexity.\n",
    "\n",
    "4. Model Training:\n",
    "\n",
    "Feed the training data into the selected model.\n",
    "The model learns relationships between features and labels through iterative optimization.\n",
    "\n",
    "5. Model Evaluation:\n",
    "\n",
    "Use the testing data to assess the model's performance.\n",
    "Metrics like accuracy, precision, recall, and F1-score help evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a1341",
   "metadata": {},
   "source": [
    "4. Go through the SVM model in depth using various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44fd0b",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    Support Vector Machine (SVM) is a powerful machine learning algorithm used for both classification and regression tasks. Let's go through it with various scenarios:\n",
    "\n",
    "1. Binary Classification:\n",
    "\n",
    "Given labeled data with two classes, SVM finds the hyperplane that best separates the classes.\n",
    "It aims to maximize the margin between the hyperplane and the closest data points (support vectors).\n",
    "If the data isn't linearly separable, SVM can use kernel functions (e.g., polynomial, radial basis function) to map it into a higher-dimensional space.\n",
    "\n",
    "2. Multi-Class Classification:\n",
    "\n",
    "SVM can handle multi-class classification using one-vs-one or one-vs-all strategies.\n",
    "In one-vs-one, SVM trains a binary classifier for each pair of classes and combines their decisions.\n",
    "In one-vs-all, it trains one classifier per class, treating all other classes as a single \"non-target\" class.\n",
    "\n",
    "3. Outlier Handling:\n",
    "\n",
    "SVM is less sensitive to outliers due to its focus on support vectors.\n",
    "Outliers can influence the hyperplane less if they're not support vectors.\n",
    "\n",
    "4. Non-Linear Separation:\n",
    "\n",
    "SVM can handle non-linearly separable data by mapping it to a higher-dimensional space using kernels.\n",
    "Kernels transform the data to a space where linear separation is possible.\n",
    "Example: Mapping 2D data to 3D using a polynomial kernel.\n",
    "\n",
    "5. Parameter Tuning:\n",
    "\n",
    "SVM has hyperparameters like C (regularization parameter) and the kernel choice.\n",
    "Small C values allow more misclassifications for a wider margin, while larger C values reduce misclassifications but lead to narrower margins.\n",
    "Kernel choice depends on data characteristics. Linear kernel for linearly separable data, and others for non-linear cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ed188",
   "metadata": {},
   "source": [
    "5. What are some of the benefits and drawbacks of SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26e40c",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "   Benefits of SVM:\n",
    "\n",
    "1. Effective in High-Dimensional Spaces:\n",
    "SVM works well even in high-dimensional feature spaces, making it suitable for complex data.\n",
    "\n",
    "2. Robust to Overfitting: \n",
    "SVM's regularization helps prevent overfitting by controlling the trade-off between fitting the data and having a wider margin.\n",
    "\n",
    "3. Kernel Flexibility:\n",
    "SVM can handle non-linearly separable data by using different kernel functions.\n",
    "\n",
    "4. Global Optimal Solution:\n",
    "SVM aims to find the optimal hyperplane with the maximum margin, resulting in a global solution.\n",
    "\n",
    "5. Less Sensitivity to Outliers: \n",
    "SVM's decision boundary is influenced less by outliers due to its focus on support vectors.\n",
    "\n",
    "     Drawbacks of SVM:\n",
    "\n",
    "1. Computationally Intensive: \n",
    "Training time and memory usage can be high, especially for large datasets and non-linear kernels.\n",
    "\n",
    "2. Parameter Sensitivity:\n",
    "The performance of SVM can be sensitive to the choice of hyperparameters like C and the kernel type.\n",
    "\n",
    "3. No Probability Estimates Directly:\n",
    "SVM doesn't provide probability estimates for class membership directly; additional methods like Platt scaling are needed.\n",
    "\n",
    "4. Difficulty with Large Datasets:\n",
    "SVM might become inefficient with very large datasets, making it less suitable for big data scenarios.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c4a28",
   "metadata": {},
   "source": [
    "6. Go over the kNN model in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9da799",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans: \n",
    "   \n",
    "      k-Nearest Neighbors (kNN) Model:\n",
    "\n",
    "1.Concept: Supervised machine learning algorithm for classification and regression tasks.\n",
    "\n",
    "2.Working Principle: Predicts the label or value of a data point by considering its k nearest neighbors in the training dataset.\n",
    "\n",
    "3.Distance Metric: Typically uses Euclidean distance, but other metrics like Manhattan or cosine distance can be employed.\n",
    "\n",
    "4.Training: No explicit training phase; kNN stores the training dataset for reference.\n",
    "5.Prediction (Classification):\n",
    "\n",
    "For a new data point, find its k nearest neighbors based on chosen distance metric.\n",
    "Count class labels of these neighbors and assign the majority class label as prediction.\n",
    "6.Prediction (Regression):\n",
    "\n",
    "6.For a new data point, find its k nearest neighbors.\n",
    "Calculate the average (or weighted average) of their target values as the prediction.\n",
    "Hyperparameter 'k': Determines the number of neighbors to consider.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138d72e",
   "metadata": {},
   "source": [
    "7. Discuss the kNN algorithm's error rate and validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada16e7",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    The kNN algorithm's error rate and validation error are important aspects of its performance evaluation:\n",
    "\n",
    "1. Error Rate:\n",
    "\n",
    "Error rate measures how often the kNN model makes incorrect predictions on a dataset.\n",
    "It's calculated as the number of incorrect predictions divided by the total number of predictions.\n",
    "Lower error rate indicates better model performance.\n",
    "\n",
    "2. Validation Error:\n",
    "\n",
    "Validation error assesses how well the kNN model generalizes to new, unseen data.\n",
    "It's typically estimated using techniques like cross-validation or a separate validation dataset.\n",
    "Validation error helps in selecting the optimal value of 'k' and understanding model behavior.\n",
    "Both error rate and validation error serve as indicators of the kNN model's accuracy and its ability to generalize to new data. Reducing these errors often involves tuning hyperparameters and addressing issues such as overfitting or underfitting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6855d",
   "metadata": {},
   "source": [
    "8. For kNN, talk about how to measure the difference between the test and training results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef3693",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    To measure the difference between test and training results in kNN:\n",
    "\n",
    "1. Prediction Discrepancy:\n",
    "\n",
    "Compare predicted labels or values of test instances with their true labels or values.\n",
    "Calculate the error or difference metric (e.g., Mean Squared Error for regression, Misclassification Rate for classification).\n",
    "A higher discrepancy suggests potential overfitting or generalization issues.\n",
    "\n",
    "2. Cross-Validation:\n",
    "\n",
    "Split the dataset into training and validation sets multiple times.\n",
    "Compute the average error across these splits to gauge model performance.\n",
    "Helps estimate how well the model generalizes and identify consistency of results.\n",
    "Both methods assess the gap between kNN's performance on training vs. unseen data, aiding in evaluating its ability to make accurate predictions beyond the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef77ea",
   "metadata": {},
   "source": [
    "9. Create the kNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4790f",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    here's a concise outline of the k-Nearest Neighbors (kNN) algorithm:\n",
    "\n",
    "Input:\n",
    "\n",
    "Training dataset with labeled instances.\n",
    "New, unlabeled data point for prediction.\n",
    "1. Choose 'k':\n",
    "\n",
    "Determine the number of nearest neighbors to consider (hyperparameter 'k').\n",
    "2. Distance Metric:\n",
    "\n",
    "Select a distance metric (e.g., Euclidean, Manhattan) to measure similarity.\n",
    "3. Calculate Distances:\n",
    "\n",
    "Compute the distance between the new data point and all instances in the training dataset.\n",
    "4. Find Neighbors:\n",
    "\n",
    "Select the 'k' instances with the shortest distances as nearest neighbors.\n",
    "5. Classification:\n",
    "\n",
    "For classification, count the occurrences of each class label among the 'k' neighbors.\n",
    "Assign the class label with the highest count as the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acd735",
   "metadata": {},
   "source": [
    "10. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4dd20f",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    \n",
    "    A decision tree is a supervised machine learning algorithm used for classification and regression tasks. It recursively divides the data into subsets based on feature values to make predictions.\n",
    "\n",
    "  1.Root Node:\n",
    "Represents the entire dataset, selects the best feature to split based on certain criterion (e.g., Gini impurity, entropy).\n",
    "\n",
    "2. Internal Nodes:\n",
    "\n",
    "Test a specific feature's value and guide data down different branches.\n",
    "Each internal node poses a binary decision question based on a chosen feature.\n",
    "3. Leaf Nodes:\n",
    "\n",
    "Terminal nodes with final predictions or outcomes.\n",
    "Contains the predicted class label (classification) or value (regression) of the data.\n",
    "4. Splitting Criteria:\n",
    "\n",
    "5. Measures the impurity of a set of instances.\n",
    "Common criteria include Gini impurity and entropy for classification, variance reduction for regression.\n",
    "6. Pruning:\n",
    "\n",
    "Process of removing nodes to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c6f2f",
   "metadata": {},
   "source": [
    "11. Describe the different ways to scan a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a1130",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "    There are primarily two ways to traverse or scan a decision tree:\n",
    "\n",
    "1. Depth-First Traversal:\n",
    "\n",
    "Start at the root node and explore as far down a branch as possible before backtracking.\n",
    "Three common strategies: Pre-order, In-order, Post-order.\n",
    "Used for making predictions, visualization, and tree analysis.\n",
    "\n",
    "2. Breadth-First Traversal:\n",
    "\n",
    "Explore nodes level by level, moving horizontally across the tree.\n",
    "Useful for understanding the tree's structure and breadth, not often used for predictions.\n",
    "Both traversal methods provide insights into the tree's structure and can be employed for different purposes, such as prediction, analysis, or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcae5a",
   "metadata": {},
   "source": [
    "12. Describe in depth the decision tree algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f662005",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    \n",
    "    \n",
    "Certainly, here's a concise description of the decision tree algorithm:\n",
    "\n",
    "1. Initialization:\n",
    "\n",
    "Start with the entire dataset at the root node.\n",
    "Calculate the impurity or variance of the dataset.\n",
    "2. Splitting:\n",
    "\n",
    "Choose the best feature and value to split the dataset, aiming to reduce impurity or variance.\n",
    "Common metrics: Gini impurity, entropy, variance reduction.\n",
    "Create child nodes for each possible outcome of the split.\n",
    "3. Recursion:\n",
    "\n",
    "For each child node, repeat the splitting process if stopping criteria aren't met:\n",
    "Stopping criteria: Maximum depth reached, minimum samples per leaf, impurity threshold.\n",
    "If stopping criteria met, create a leaf node with the majority class label (classification) or average value (regression).\n",
    "4. Pruning (Optional):\n",
    "\n",
    "Post-process to remove nodes that don't contribute significantly to prediction accuracy.\n",
    "Helps prevent overfitting by simplifying the tree.\n",
    "5. Prediction:\n",
    "\n",
    "For a new instance, traverse the tree based on feature values.\n",
    "Reach a leaf node and return its predicted class label (classification) or value (regression).\n",
    "6. Feature Importance:\n",
    "\n",
    "Calculate feature importance by measuring the contribution of each feature's splits to the model's performance.\n",
    "Features with higher importance are more influential in making decisions.\n",
    "Ensemble Methods:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2a5fa",
   "metadata": {},
   "source": [
    "13. In a decision tree, what is inductive bias? What would you do to stop overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947602f",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "1. Inductive Bias in Decision Trees:\n",
    "\n",
    "Inductive bias is the set of assumptions or prior knowledge that guides a machine learning algorithm's learning process.\n",
    "In decision trees, the inductive bias is that simpler and shorter trees are preferred over complex ones.\n",
    "2. Stopping Overfitting in Decision Trees:\n",
    "\n",
    "Overfitting occurs when the tree captures noise or small fluctuations in the training data.\n",
    "To prevent overfitting:\n",
    "Limit the maximum depth of the tree.\n",
    "Set a minimum number of samples required to split a node (min_samples_split).\n",
    "Require a minimum number of samples in a leaf (min_samples_leaf).\n",
    "Set a maximum number of leaf nodes (max_leaf_nodes).\n",
    "Prune the tree by removing nodes that don't contribute significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a13b4",
   "metadata": {},
   "source": [
    "14. Explain advantages and disadvantages of using a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a9b6e",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    \n",
    "    Advantages of Using a Decision Tree:\n",
    "\n",
    "1.Interpretability: Decision trees are easy to understand and interpret, making them suitable for non-technical users.\n",
    "\n",
    "2.Handling Non-linearity: They can model complex relationships without requiring linear assumptions, accommodating nonlinear data.\n",
    "\n",
    "3.Feature Selection: Decision trees automatically select important features, reducing the need for extensive feature engMixed 4.Data Types: They can handle both numerical and categorical data without requiring extensive preprocessing.\n",
    "\n",
    "    Disadvantages of Using a Decision Tree:\n",
    "\n",
    "1.Overfitting: Decision trees can easily overfit noisy data, leading to poor generalization on unseen data.\n",
    "\n",
    "2.Instability: Small changes in the data can lead to different tree structures, making them unstable.\n",
    "\n",
    "3.Bias towards Dominant Classes: In classification tasks, decision trees tend to favor dominant classes, leading to imbalanced predictions.\n",
    "\n",
    "4 High Variance: Unconstrained trees can have high variance, resulting in sensitivity to minor data changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae42923",
   "metadata": {},
   "source": [
    "15. Describe in depth the problems that are suitable for decision tree learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d18b87",
   "metadata": {},
   "source": [
    "Ans: \n",
    "    \n",
    "    Decision tree learning is suitable for a variety of problems, particularly when the data and task characteristics align with its strengths:\n",
    "\n",
    "1.Categorical and Numerical Data: Decision trees handle both categorical and numerical features without the need for extensive preprocessing.\n",
    "\n",
    "2.Nonlinear Relationships: They excel in capturing nonlinear patterns in data, making them suitable for tasks where relationships aren't linear.\n",
    "\n",
    "3.Feature Interactions: Decision trees can model interactions between features, allowing them to capture complex dependencies.\n",
    "\n",
    "4.Interpretable Models: When interpretability is crucial, decision trees provide clear, visual explanations of decisions.\n",
    "\n",
    "5.Binary and Multiclass Classification: They can handle both binary and multiclass classification problems effectively.\n",
    "\n",
    "6.Feature Importance: Decision trees naturally indicate feature importance by their position and depth in the tree.\n",
    "\n",
    "7.Missing Data Handling: They can handle missing values gracefully, without requiring imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb05cf",
   "metadata": {},
   "source": [
    "16. Describe in depth the random forest model. What distinguishes a random forest?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4714f9",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "    A Random Forest is an ensemble learning technique that combines multiple decision trees to improve predictive accuracy and control overfitting. Here's a concise description of what distinguishes a Random Forest:\n",
    "\n",
    "1.Ensemble of Decision Trees: A Random Forest consists of multiple decision trees, each trained on a different subset of the data using bootstrapped sampling.\n",
    "\n",
    "2.Random Feature Selection: At each node of every tree, only a random subset of features is considered for splitting. This reduces correlation between trees and enhances diversity.\n",
    "\n",
    "3.Voting or Averaging: For classification tasks, the mode (most common prediction) of the individual tree predictions is taken as the final prediction. For regression tasks, the average of individual tree predictions is used.\n",
    "\n",
    "4.Reduced Overfitting: The combination of diverse trees and random feature selection helps mitigate overfitting, resulting in improved generalization to unseen data.\n",
    "\n",
    "5.Bagging Technique: Random Forest employs a bagging (bootstrap aggregating) approach, where each tree is trained on a bootstrapped sample of the training data.\n",
    "\n",
    "6.Out-of-Bag (OOB) Sampling: Some data points are left out during the bootstrapping process. These OOB samples can be used to validate and assess the performance of the model without needing a separate validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b1d2a",
   "metadata": {},
   "source": [
    "17. In a random forest, talk about OOB error and variable value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117cd171",
   "metadata": {},
   "source": [
    "Ans: \n",
    "       OOB Error (Out-of-Bag Error):\n",
    "In a Random Forest, the OOB error is the prediction error of the model on the data points that were not included in the bootstrap sample used to train each individual decision tree. Since these data points were not seen during the training of a particular tree, they serve as a validation set for that tree. The OOB error is calculated by aggregating the predictions from all trees on their respective OOB samples and comparing them to the true labels. It provides an estimate of the model's performance without the need for a separate validation set.\n",
    "\n",
    "    Variable Importance:\n",
    "Variable importance in a Random Forest measures the contribution of each feature (variable) in the model's predictive performance. It is determined by evaluating how much the accuracy of the model decreases when the values of a particular feature are randomly permuted while keeping other features constant. The larger the decrease in accuracy, the more important the feature is considered. Variable importance scores are calculated across all trees in the forest and can help identify which features are most influential in making accurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
